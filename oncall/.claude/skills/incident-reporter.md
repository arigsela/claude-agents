---
name: incident-reporter
description: Generate detailed incident reports for Kubernetes issues in the homelab cluster. Use when investigating incidents, creating post-mortems, or documenting troubleshooting actions.
capabilities: document_generation
version: 1.0.0
---

# Incident Reporter Skill

This skill generates standardized incident reports for Kubernetes issues in Ari's K3s homelab cluster. It provides professional documentation of incidents, investigations, and remediation actions.

## When to Use This Skill

- **During Incident Investigation**: Generate reports while troubleshooting active incidents
- **Post-Incident Documentation**: Create detailed post-mortems after incidents are resolved
- **Status Updates**: Document current state of ongoing incidents
- **Knowledge Base**: Build historical record of cluster issues and resolutions

## Available Templates

### k8s_incident_report

Comprehensive incident report with full investigation timeline, root cause analysis, and remediation steps.

**Required Variables**:
- `timestamp` (str): ISO format timestamp of incident/report generation
- `cluster_name` (str): Kubernetes cluster name (e.g., "homelab-k3s")
- `severity` (str): Incident severity level (critical|high|medium|low)
- `summary` (str): Brief incident description (1-2 sentences)
- `affected_resources` (list): Resources impacted by incident
- `investigation_steps` (list): Steps taken during investigation
- `root_cause` (str): Root cause analysis summary
- `remediation_actions` (list): Actions taken to resolve incident
- `recommendations` (str): Recommendations to prevent recurrence

**Optional Variables**:
- `follow_up_required` (bool): Whether follow-up actions needed (default: false)
- `follow_up_details` (str): Details of required follow-up

```template:k8s_incident_report
# Kubernetes Incident Report

**Generated**: {{timestamp}}
**Cluster**: {{cluster_name}}
**Severity**: {{severity | upper}}

---

## Incident Summary

{{summary}}

---

## Affected Resources

{% if affected_resources %}
{% for resource in affected_resources %}
### {{resource.kind}}: `{{resource.name}}`
- **Namespace**: {{resource.namespace}}
- **Status**: {{resource.status}}
- **Issue**: {{resource.issue}}
{% if resource.additional_info %}
- **Additional Info**: {{resource.additional_info}}
{% endif %}
{% endfor %}
{% else %}
No resources specified.
{% endif %}

---

## Investigation Timeline

{% if investigation_steps %}
{% for step in investigation_steps %}
### Step {{loop.index}}: {{step.title}}
**Time**: {{step.timestamp}}
**Action**: {{step.description}}

{% if step.command %}
**Command Executed**:
```bash
{{step.command}}
```
{% endif %}

{% if step.output %}
**Output**:
```
{{step.output | truncate(500)}}
```
{% endif %}

{% if step.findings %}
**Findings**: {{step.findings}}
{% endif %}

{% endfor %}
{% else %}
No investigation steps documented.
{% endif %}

---

## Root Cause Analysis

{{root_cause}}

---

## Remediation Actions

{% if remediation_actions %}
{% for action in remediation_actions %}
{{loop.index}}. **[{{action.status | upper}}]** {{action.description}}
{% if action.command %}
   - Command: `{{action.command}}`
{% endif %}
{% if action.result %}
   - Result: {{action.result}}
{% endif %}
{% if action.timestamp %}
   - Time: {{action.timestamp}}
{% endif %}
{% endfor %}
{% else %}
No remediation actions taken yet.
{% endif %}

---

## Recommendations

{{recommendations}}

---

## Follow-up Required

{% if follow_up_required %}
⚠️ **Yes - Follow-up actions required**

{{follow_up_details}}
{% else %}
✅ **No** - Incident fully resolved. No follow-up required.
{% endif %}

---

## Related Links

- [Homelab K8s Failure Patterns](../.claude/skills/k8s-failure-patterns.md)
- [Homelab Runbooks](../.claude/skills/homelab-runbooks.md)

---

*Report generated by OnCall Agent - Incident Reporter Skill v1.0.0*
```

---

## Usage Examples

### Example 1: OOMKilled Pod Incident

```python
data = {
    "timestamp": "2025-10-23T12:15:30Z",
    "cluster_name": "homelab-k3s",
    "severity": "high",
    "summary": "chores-tracker pod terminated due to OOMKilled. Database backup process consumed excessive memory during large dataset export.",
    "affected_resources": [
        {
            "kind": "Pod",
            "name": "chores-tracker-backend-7d8f9c-xk2lm",
            "namespace": "homelab-prod",
            "status": "OOMKilled",
            "issue": "Memory limit exceeded during backup operation"
        }
    ],
    "investigation_steps": [
        {
            "title": "Check pod status",
            "timestamp": "2025-10-23T12:16:00Z",
            "description": "Verified pod was terminated with OOMKilled status",
            "command": "kubectl describe pod chores-tracker-backend-7d8f9c-xk2lm -n homelab-prod",
            "findings": "Last State: Terminated (Reason: OOMKilled, Exit Code: 137)"
        },
        {
            "title": "Review pod logs",
            "timestamp": "2025-10-23T12:17:30Z",
            "description": "Checked application logs before termination",
            "command": "kubectl logs chores-tracker-backend-7d8f9c-xk2lm -n homelab-prod --previous",
            "output": "[2025-10-23 12:15:15] Starting weekly backup export\\n[2025-10-23 12:15:25] Processing 150,000 records\\n[2025-10-23 12:15:29] FATAL: Out of memory",
            "findings": "Backup process loaded entire dataset into memory"
        },
        {
            "title": "Check resource limits",
            "timestamp": "2025-10-23T12:18:00Z",
            "description": "Reviewed current memory limits",
            "command": "kubectl get pod chores-tracker-backend-7d8f9c-xk2lm -n homelab-prod -o jsonpath='{.spec.containers[0].resources.limits.memory}'",
            "output": "256Mi",
            "findings": "Memory limit is 256Mi, insufficient for large exports"
        }
    ],
    "root_cause": "The chores-tracker backup process loads the entire dataset into memory before exporting. With 150,000 records, this exceeds the current 256Mi memory limit. The backup script should be modified to use streaming/batched exports.",
    "remediation_actions": [
        {
            "status": "completed",
            "description": "Restart pod to restore service",
            "command": "kubectl delete pod chores-tracker-backend-7d8f9c-xk2lm -n homelab-prod",
            "result": "Pod recreated, service restored",
            "timestamp": "2025-10-23T12:20:00Z"
        },
        {
            "status": "completed",
            "description": "Temporarily increased memory limit to 512Mi",
            "command": "kubectl set resources deployment chores-tracker-backend -n homelab-prod --limits=memory=512Mi",
            "result": "Deployment updated, pods rolling restart",
            "timestamp": "2025-10-23T12:22:00Z"
        }
    ],
    "recommendations": "1. Modify backup script to use streaming/batched exports instead of loading full dataset\\n2. Add memory usage monitoring alerts for backup operations\\n3. Consider moving large exports to dedicated cron job with higher limits\\n4. Document backup memory requirements in service runbook",
    "follow_up_required": True,
    "follow_up_details": "- Create GitHub issue to refactor backup script for streaming exports\\n- Add Prometheus alert for backup process memory usage > 80%\\n- Update chores-tracker deployment docs with backup memory requirements"
}
```

### Example 2: CrashLoopBackOff Incident

```python
data = {
    "timestamp": "2025-10-23T14:30:00Z",
    "cluster_name": "homelab-k3s",
    "severity": "critical",
    "summary": "MySQL database pod in CrashLoopBackOff due to corrupted InnoDB tablespace after unclean shutdown.",
    "affected_resources": [
        {
            "kind": "Pod",
            "name": "mysql-0",
            "namespace": "homelab-prod",
            "status": "CrashLoopBackOff",
            "issue": "Database startup failing with InnoDB corruption errors",
            "additional_info": "Backoff delay: 5m"
        },
        {
            "kind": "Service",
            "name": "chores-tracker",
            "namespace": "homelab-prod",
            "status": "Degraded",
            "issue": "Unable to connect to MySQL database"
        }
    ],
    "investigation_steps": [
        {
            "title": "Check MySQL pod logs",
            "timestamp": "2025-10-23T14:31:00Z",
            "description": "Reviewed startup failure logs",
            "command": "kubectl logs mysql-0 -n homelab-prod --previous",
            "output": "InnoDB: Database page corruption on tablespace chores_tracker page 12345\\nInnoDB: Cannot start InnoDB. See error log.",
            "findings": "InnoDB detected corrupted page in chores_tracker tablespace"
        },
        {
            "title": "Verify backup availability",
            "timestamp": "2025-10-23T14:32:00Z",
            "description": "Checked for recent S3 backups",
            "findings": "Last successful backup: 2025-10-23 02:00:00 UTC (12 hours ago)"
        }
    ],
    "root_cause": "MySQL pod experienced unclean shutdown (node reboot without graceful pod termination), resulting in InnoDB tablespace corruption. The database cannot start with corrupted pages.",
    "remediation_actions": [
        {
            "status": "completed",
            "description": "Attempted InnoDB force recovery mode",
            "result": "Recovery mode failed - corruption too severe",
            "timestamp": "2025-10-23T14:35:00Z"
        },
        {
            "status": "completed",
            "description": "Restored database from S3 backup (2025-10-23 02:00:00)",
            "result": "Database restored successfully, 12 hours of data loss",
            "timestamp": "2025-10-23T14:45:00Z"
        },
        {
            "status": "completed",
            "description": "Verified database integrity and restarted pod",
            "result": "MySQL started successfully, accepting connections",
            "timestamp": "2025-10-23T14:50:00Z"
        }
    ],
    "recommendations": "1. Implement PersistentVolume with proper fsync settings\\n2. Configure pod disruption budgets to prevent abrupt terminations\\n3. Increase backup frequency from daily to every 4 hours\\n4. Add automated InnoDB corruption detection to monitoring",
    "follow_up_required": True,
    "follow_up_details": "- **CRITICAL**: Notify users of 12-hour data loss (2025-10-23 02:00-14:00)\\n- Investigate root cause of unclean node shutdown\\n- Implement more frequent backups (4-hour intervals)\\n- Review and improve MySQL StatefulSet configuration for durability"
}
```

---

## Best Practices

### 1. Investigation Step Documentation

Always document investigation steps in chronological order with:
- Clear step titles
- Timestamps for each action
- Commands executed (for reproducibility)
- Key outputs (truncated if very long)
- Findings and insights

### 2. Root Cause Analysis

Provide clear, concise root cause that explains:
- What happened (symptom)
- Why it happened (cause)
- What enabled it to happen (contributing factors)

### 3. Remediation Actions

Document each action with:
- Status: completed, pending, or failed
- Description of what was done
- Commands executed
- Results/outcomes
- Timestamps

### 4. Recommendations

Focus on prevention:
- Configuration changes
- Monitoring improvements
- Process updates
- Documentation needs

### 5. Follow-up Tracking

If follow-up required, specify:
- Concrete action items
- Owners (if known)
- Priority/urgency
- Links to created tickets/issues

---

## Integration with Agent

When the OnCall agent is investigating an incident, it should:

1. **Gather Data**: Use existing tools (k8s queries, logs, GitHub) to collect investigation data
2. **Structure Data**: Organize findings into the template variables format
3. **Call Tool**: Use `generate_skill_document` tool with:
   - `skill_name`: "incident-reporter"
   - `template_name`: "k8s_incident_report"
   - `data`: Structured incident data
4. **Save Report**: Report saved to `/tmp/oncall-reports/`
5. **Notify**: Attach report path to Slack notification

---

## Output Location

Reports are saved to: `/tmp/oncall-reports/`

Filename format: `k8s_incident_report-{service}-{timestamp}.md`

Example: `k8s_incident_report-chores-tracker-2025-10-23-121530.md`

---

## Version History

- **v1.0.0** (2025-10-23): Initial release
  - k8s_incident_report template
  - Support for full incident documentation
  - Integration with homelab K8s failure patterns

---

*Skill maintained by: Ari Sela (DevOps Engineering)*
